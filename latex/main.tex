\documentclass{article}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{siunitx}
\pgfplotsset{compat=1.6}
\usepackage[margin=1.4in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[hidelinks,pdftex]{hyperref}
\hypersetup{
 colorlinks,
 citecolor=blue,
 linkcolor=blue,
 urlcolor=blue}
\usepackage[nameinlink, noabbrev]{cleveref}

\lstset{language=C++,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    morekeywords={uint64_},
    morecomment=[l][\color{magenta}]{\#}
}
\title{Problem set 4 - TMA4280}
\author{Morten Vassvik and Jon Vegard Ven{\aa}s}
\date{February 2016}

\begin{document}

\maketitle

The serial code is implemented in \lstinline$compute_S.c$. The usage of \lstinline$uint64_t$ (from the \lstinline$stdint.h$ library, a typedef corresponding to \lstinline$unsigned long long int$) is included to be able to study $n=2^k$ up to\footnote{Note that the dominant restriction on the data type for the variable $n$ is when we compute $\frac{1}{n^2}$ in the final term in the sum. With 64 bits available the product \lstinline$n*n$ makes sence for $n=2^{31}$.} $k=31$. The corresponding capacity for \lstinline$unsigned int$ and \lstinline$unsigned short$ (32-bit and 16-bit) are $k=15$ and $k=9$. Moreover, we include the \lstinline$time.h$ library to calculate computational time.

Note that we compute the sum
\begin{equation*}
    S_n = \sum_{i=1}^n \frac{1}{i^2}
\end{equation*}
by adding the terms in reversed order to obtain the optimal accuracy\footnote{Adding numbers of similar exponents increase accuracy.}.

The OpenMP parallelization is obtained from the serial code by adding \lstinline$#pragma$ commands above each \lstinline$for$-loop. We substitute the \lstinline$time.h$ library with \lstinline$omp.h$ library to compute the computational wall time with \lstinline$omp_get_wtime()$. Additional arguments to the \lstinline$#pragma$ are used to specify intent and hints, such as shared and private variables,  if parallellization are to be performed across several loops, or if a reduction to be performed. 

Some results for the MPI implementation is reported in \Cref{Tab:results1}, while the full set of results is plotted in \Cref{Fig:firstPlot}. All though process zero should be responsable for generating the vector elements, there is no need to allocate the full vector as each sub vector can be created, sent and overrided for the next sub vector. This allows for an evenly distribution of memory across the processes, such that the memory requirement for each process is reduced by a factor $P$ (number of processes) compared to the single-process program.

\begin{table}
	\centering
	\caption{Comparison of the error $|S-S_n|$ for different values of $k$ and number of processors $P$.}
	\label{Tab:results1}
	\pgfplotstableset{% global config, for example in the preamble
	% these columns/<colname>/.style={<options>} things define a style
	% which applies to <colname> only.
	columns/k/.style={int detect,column type=r,column name=\textsc{$k$}}, 
	columns/Serial/.style={/pgf/number format/sci, sci sep align, column name={Serial}},
	columns/OpenMP/.style={/pgf/number format/sci, sci sep align, column name={OpenMP}},
	columns/MPI/.style={/pgf/number format/sci, sci sep align, column name={MPI}},
	every head row/.style={before row=\toprule,after row=\midrule},
	every last row/.style={after row=\bottomrule}
	}
	\pgfplotstabletypeset[fixed zerofill,precision=6, 1000 sep={\,},1000 sep in fractionals]{exercise3_table.txt}
\end{table}

\begin{figure}
	\begin{tikzpicture}
		\begin{loglogaxis}[
			width = 0.95\textwidth,
			height = 0.3\paperheight,
			cycle list={
				{red, mark=square*},
				{green, mark=diamond*},
				{blue, mark=triangle*},
				{black, dashed},
			},
			xlabel={$n=2^k$},
			ylabel=Absolute relative error (\%),
			]	    
			\addplot table[x expr={2^\thisrowno{0}},y=|S-S_n|] {exercise3_plot_compute_S.txt};
			\addlegendentry{Serial} 
			\addplot table[x expr={2^\thisrowno{0}},y=|S-S_n|] {exercise3_plot_compute_S_OpenMP.txt};
			\addlegendentry{Open MP} 
			\addplot table[x expr={2^\thisrowno{0}},y=|S-S_n|] {exercise3_plot_compute_S_MPI.txt};
			\addlegendentry{MPI} 
			\addplot table[x expr={2^\thisrowno{0}},y expr={2*2^(-\thisrowno{0})}] {exercise3_plot_compute_S.txt};
			\addlegendentry{Slope -1} 
		\end{loglogaxis}
	\end{tikzpicture}
	\caption{Plot of the error $|S-S_n|$ for $k=3,\dots,31$ for the three different implementations.}
	\label{Fig:firstPlot}
\end{figure}

As before, we can use MPI and OpenMP in combination by extending the MPI implementation with \lstinline$#pragma$ commands above each \lstinline$for$-loop. Note that there is no need to declare the index \lstinline$i$ as private, as it is declared inside the \lstinline$for$-loop. 

We had to use \lstinline$MPI_Send()$ from rank 0 to send the vectors to all other processes, who used \lstinline$MPI_Recv()$ to recieve the same data. \lstinline$MPI_Reduce$ was used to perform the sum of the partial sums, but are not nescessary, as the same results could be done with Send/Recv. \lstinline$MPI_Wtime()$ is convenient to get the wall-time. We also need to use the standard initialization and finalization routines, \lstinline$MPI_Init$, \lstinline$MPI_Comm_size$, \lstinline$MPI_Comm_rank$, and \lstinline$MPI_Finalize$.  

We could have included non blocking functions \lstinline$MPI_Isend()$ and \lstinline$MPI_Irecv()$ to let the rank 0 process continue without waiting for \lstinline$MPI_Recv()$ to return, but as this would require a barrier elsewhere and not give significant improvements, we chose not to.

The answers should be approximitely the same, but not exactly, due to the accumulation of floating point errors. In addition, the order in which a sum is performed will change its value. This is illustrated in \Cref{Tab:tab2}.

\begin{table}
	\centering
	\caption{Comparison of the error $|S-S_n|$ for different values of $k$ and number of processors $P$.}
	\label{Tab:tab2}
	\pgfplotstableset{% global config, for example in the preamble
	% these columns/<colname>/.style={<options>} things define a style
	% which applies to <colname> only.
	columns/k/.style={int detect,column type=r,column name=\textsc{$k$}}, 
	columns/MPI_1/.style={/pgf/number format/sci, sci sep align, column name={$P=1$}},
	columns/MPI_2/.style={/pgf/number format/sci, sci sep align, column name={$P=2$}},
	columns/MPI_8/.style={/pgf/number format/sci, sci sep align, column name={$P=8$}},
	every head row/.style={before row=\toprule,after row=\midrule},
	every last row/.style={after row=\bottomrule}
	}
	\pgfplotstabletypeset[fixed zerofill,precision=15, 1000 sep={\,},1000 sep in fractionals]{exercise6.txt}
\end{table}


For each of the $n$ elements in the vector $\boldsymbol{v}$, we perform an integer multiplication, a type cast (from int to double) and a floating point division. Moreover, the iteration index will be incremented and checked against $n$.

The vector generation is done on only one processor (for MPI), which does not result in a load balanced program as the vectorgeneration consumes about the same time as the summation. It is possible to solve this problem if each process generated their own part of the vector $\boldsymbol{v}$. % (spørsmåletsnakker kun om MPI) The OpenMP implementation however is not bound by this problem.

Summing a series of numbers is ideal for parallel computing, but it depends on the problem itself. In this case, the series is so slowly converging that there is a huge accumulation of floating point errors. This results in the result unpredictable and varying precision in the final results, as the order in which the sum is performed is important. Moreover, for the MPI implementation, it should be more load balanced.

%As the numbers must be added sequencially (from i = n to i = 1) to obtain optimal accuracy parallell processsing is not so well suited for this problem. 


\end{document}
